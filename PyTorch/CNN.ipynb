{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001\n",
    "DOWNLOAD_MNIST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(root = \"./data/\",\n",
    "                                        transform = transforms.ToTensor(),\n",
    "                            train = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.train_data.size())\n",
    "print(train_data.train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'5')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADylJREFUeJzt3X2MXPV1xvHniV9rY4IdB9chLjjgBAg0Jl0ZEBZQoRAHVQJUBWKhyKG0ThOclNaVoLQqtKKtWyVEDqFIpriYivcEhKVSEmqlkLTBZaEGzDsY09gYG+OCgYBf1qd/7DjawM5v1zN39o59vh9ptDP33Dv3aODxnZnfnftzRAhAPh+quwEA9SD8QFKEH0iK8ANJEX4gKcIPJEX4gaQIPwZl+z9sv2f77cbt2bp7QrUIP0oWRcRBjdun6m4G1SL8QFKEHyV/Z3ur7f+0fXrdzaBa5tx+DMb2iZKekrRT0pckfU/S7Ih4sdbGUBnCj2GxfZ+kf42Ia+ruBdXgbT+GKyS57iZQHcKPD7B9iO3P2x5ve7TtCySdKum+untDdUbX3QC60hhJV0k6WlKfpGcknRMRz9XaFSrFZ34gKd72A0kRfiApwg8kRfiBpEb02/6xHhfjNXEkdwmk8p7e0c7YMazzMdoKv+15kpZKGiXpnyJiSWn98ZqoE31GO7sEULA6Vg173Zbf9tseJelaSV+QdKyk+baPbfX5AIysdj7zz5H0QkSsi4idkm6TdHY1bQHotHbCf5iknw94vKGx7FfYXmi713bvLu1oY3cAqtTxb/sjYllE9EREzxiN6/TuAAxTO+HfKGnGgMcfbywDsB9oJ/wPS5ple6btseq/4MPKatoC0GktD/VFxG7biyT9UP1Dfcsj4snKOgPQUW2N80fEvZLuragXACOI03uBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iqq1ZetH9PLr8n3jUR6d2dP/P/ukRTWt9E/YUtz38yC3F+oSvu1h/9eqxTWuP9txe3HZr3zvF+ol3Li7Wj/qTh4r1btBW+G2vl/SWpD5JuyOip4qmAHReFUf+346IrRU8D4ARxGd+IKl2wx+SfmT7EdsLB1vB9kLbvbZ7d2lHm7sDUJV23/bPjYiNtg+VdL/tZyLiwYErRMQyScsk6WBPiTb3B6AibR35I2Jj4+8WSXdLmlNFUwA6r+Xw255oe9Le+5LOlLS2qsYAdFY7b/unSbrb9t7nuSUi7qukqwPMqGNmFesxbkyx/spphxTr757UfEx6yofL49U/+Ux5vLtO//aLScX6339vXrG++vhbmtZe2vVucdslmz9XrH/sJ/v/J9iWwx8R6yR9psJeAIwghvqApAg/kBThB5Ii/EBShB9Iip/0VqDv9M8W61ffeG2x/skxzX96eiDbFX3F+l9e85ViffQ75eG2k+9c1LQ2aePu4rbjtpaHAif0ri7W9wcc+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcb5KzDu2VeK9Ufem1Gsf3LM5irbqdTiTScV6+veLl/6+8Yjv9+09uae8jj9tO/+V7HeSfv/D3aHxpEfSIrwA0kRfiApwg8kRfiBpAg/kBThB5JyxMiNaB7sKXGizxix/XWLbReeXKxvn1e+vPaoxw8q1h/7+jX73NNeV239zWL94dPK4/h9b7xZrMfJzS/wvP6bxU01c/5j5RXwAatjlbbHtvLc5Q0c+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcb5u8CoqR8p1vte31asv3RL87H6J09dXtx2zt9+o1g/9Nr6flOPfVfpOL/t5ba32F47YNkU2/fbfr7xd3I7DQMYecN523+jpHnvW3aZpFURMUvSqsZjAPuRIcMfEQ9Kev/7zrMlrWjcXyHpnIr7AtBhrV7Db1pEbGrcf1XStGYr2l4oaaEkjdeEFncHoGptf9sf/d8YNv3WMCKWRURPRPSM0bh2dwegIq2Gf7Pt6ZLU+LulupYAjIRWw79S0oLG/QWS7qmmHQAjZcjP/LZvlXS6pKm2N0i6QtISSXfYvkjSy5LO62STB7q+ra+3tf2u7WNb3vbTFzxVrL923ajyE+zpa3nfqNeQ4Y+I+U1KnK0D7Mc4vRdIivADSRF+ICnCDyRF+IGkmKL7AHDMpc81rV14fHlQ5p8PX1Wsn/bFi4v1Sbc/VKyje3HkB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOc/AJSmyX79a8cUt/3fle8W65dddVOx/mfnnVusx/98uGltxt/8rLitRvCy8hlx5AeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpJiiO7ltv3dysX7zFd8q1meOHt/yvj9906Jifdb1m4r13evWt7zvA1WlU3QDODARfiApwg8kRfiBpAg/kBThB5Ii/EBSjPOjKE6ZXawfvGRDsX7rJ37Y8r6P/vHvF+uf+qvm1zGQpL7n17W87/1VpeP8tpfb3mJ77YBlV9reaHtN43ZWOw0DGHnDedt/o6R5gyz/TkTMbtzurbYtAJ02ZPgj4kFJ20agFwAjqJ0v/BbZfrzxsWBys5VsL7Tda7t3l3a0sTsAVWo1/NdJOlLSbEmbJH272YoRsSwieiKiZ4zGtbg7AFVrKfwRsTki+iJij6TrJc2pti0AndZS+G1PH/DwXElrm60LoDsNOc5v+1ZJp0uaKmmzpCsaj2dLCknrJX01Iso/vhbj/AeiUdMOLdZfOf+oprXVly4tbvuhIY5NF7x0ZrH+5tzXi/UD0b6M8w85aUdEzB9k8Q373BWArsLpvUBShB9IivADSRF+ICnCDyTFT3pRmzs2lKfonuCxxfovYmex/jvfuKT5c9+9urjt/opLdwMYEuEHkiL8QFKEH0iK8ANJEX4gKcIPJDXkr/qQ25655Ut3v/jF8hTdx81e37Q21Dj+UK7ZdkKxPuGe3rae/0DHkR9IivADSRF+ICnCDyRF+IGkCD+QFOEHkmKc/wDnnuOK9ee+WR5rv/6UFcX6qePLv6lvx47YVaw/tG1m+Qn2DHk1+dQ48gNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUkOO89ueIekmSdPUPyX3sohYanuKpNslHaH+abrPi4j/61yreY2eeXix/uKFH2tau/L824rb/u5BW1vqqQqXb+4p1h9YelKxPnlF+br/KBvOkX+3pMURcaykkyRdbPtYSZdJWhURsyStajwGsJ8YMvwRsSkiHm3cf0vS05IOk3S2pL2nf62QdE6nmgRQvX36zG/7CEknSFotaVpE7D1/8lX1fywAsJ8YdvhtHyTpB5IuiYjtA2vRP+HfoJP+2V5ou9d27y7taKtZANUZVvhtj1F/8G+OiLsaizfbnt6oT5e0ZbBtI2JZRPRERM8YjauiZwAVGDL8ti3pBklPR8TVA0orJS1o3F8g6Z7q2wPQKcP5Se8pkr4s6QnbaxrLLpe0RNIdti+S9LKk8zrT4v5v9BG/Uay/+VvTi/Xz//q+Yv0PD7mrWO+kxZvKw3E/+8fmw3lTbvzv4raT9zCU10lDhj8ifiqp2XzfZ1TbDoCRwhl+QFKEH0iK8ANJEX4gKcIPJEX4gaS4dPcwjZ7+601r25ZPLG77tZkPFOvzJ21uqacqLNo4t1h/9LryFN1Tv7+2WJ/yFmP13YojP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8klWacf+fny5eJ3vnH24r1y4+6t2ntzF97p6WeqrK5792mtVNXLi5ue/RfPFOsT3mjPE6/p1hFN+PIDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJpRnnX39O+d+5546/s2P7vvaNI4v1pQ+cWay7r9mV0/sdfdVLTWuzNq8ubttXrOJAxpEfSIrwA0kRfiApwg8kRfiBpAg/kBThB5JyRJRXsGdIuknSNEkhaVlELLV9paQ/kPRaY9XLI6L5j94lHewpcaKZ1RvolNWxSttjW/nEkIbhnOSzW9LiiHjU9iRJj9i+v1H7TkR8q9VGAdRnyPBHxCZJmxr337L9tKTDOt0YgM7ap8/8to+QdIKkveeMLrL9uO3ltic32Wah7V7bvbu0o61mAVRn2OG3fZCkH0i6JCK2S7pO0pGSZqv/ncG3B9suIpZFRE9E9IzRuApaBlCFYYXf9hj1B//miLhLkiJic0T0RcQeSddLmtO5NgFUbcjw27akGyQ9HRFXD1g+fcBq50oqT9cKoKsM59v+UyR9WdITttc0ll0uab7t2eof/lsv6asd6RBARwzn2/6fShps3LA4pg+gu3GGH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+IKkhL91d6c7s1yS9PGDRVElbR6yBfdOtvXVrXxK9tarK3g6PiI8OZ8URDf8Hdm73RkRPbQ0UdGtv3dqXRG+tqqs33vYDSRF+IKm6w7+s5v2XdGtv3dqXRG+tqqW3Wj/zA6hP3Ud+ADUh/EBStYTf9jzbz9p+wfZldfTQjO31tp+wvcZ2b829LLe9xfbaAcum2L7f9vONv4POkVhTb1fa3th47dbYPqum3mbY/rHtp2w/afuPGstrfe0KfdXyuo34Z37boyQ9J+lzkjZIeljS/Ih4akQbacL2ekk9EVH7CSG2T5X0tqSbIuK4xrJ/kLQtIpY0/uGcHBGXdklvV0p6u+5p2xuzSU0fOK28pHMkfUU1vnaFvs5TDa9bHUf+OZJeiIh1EbFT0m2Szq6hj64XEQ9K2va+xWdLWtG4v0L9//OMuCa9dYWI2BQRjzbuvyVp77Tytb52hb5qUUf4D5P08wGPN6jGF2AQIelHth+xvbDuZgYxLSI2Ne6/Kmlanc0MYshp20fS+6aV75rXrpXp7qvGF34fNDciPivpC5Iubry97UrR/5mtm8ZqhzVt+0gZZFr5X6rztWt1uvuq1RH+jZJmDHj88cayrhARGxt/t0i6W9039fjmvTMkN/5uqbmfX+qmadsHm1ZeXfDaddN093WE/2FJs2zPtD1W0pckrayhjw+wPbHxRYxsT5R0prpv6vGVkhY07i+QdE+NvfyKbpm2vdm08qr5teu66e4jYsRvks5S/zf+L0r68zp6aNLXJyQ91rg9WXdvkm5V/9vAXer/buQiSR+RtErS85L+XdKULurtXyQ9Ielx9Qdtek29zVX/W/rHJa1p3M6q+7Ur9FXL68bpvUBSfOEHJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0n9P5f0tV7WKdnwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data.train_data[0].numpy())\n",
    "plt.title(train_data.train_labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(train_data, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(root='./data/',train=False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1)).type(torch.FloatTensor)[:2000]/255\n",
    "test_y = test_data.test_labels[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16,32,5,1,2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output, x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr = LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   tensor(2.3008)   0.137\n",
      "0   tensor(0.3618)   0.8725\n",
      "0   tensor(0.2729)   0.8895\n",
      "0   tensor(0.1024)   0.935\n",
      "0   tensor(0.3273)   0.9585\n",
      "0   tensor(0.0449)   0.9595\n",
      "0   tensor(0.2059)   0.9645\n",
      "0   tensor(0.0308)   0.964\n",
      "0   tensor(0.1054)   0.967\n",
      "0   tensor(0.0518)   0.967\n",
      "0   tensor(0.0204)   0.9715\n",
      "0   tensor(0.0311)   0.9745\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for step, (x,y) in enumerate(train_loader):\n",
    "        b_x = Variable(x)\n",
    "        b_y = Variable(y)\n",
    "        \n",
    "        output = cnn(b_x)[0]\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            test_output, last_layer = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            acc = (pred_y == test_y).sum().item()/float(test_y.size(0))\n",
    "            print(epoch, ' ', loss.data[0], ' ', acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output, last_layer = cnn(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1,  ..., 3, 9, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.max(1)[1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
